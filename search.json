[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/hw1/hw1.html",
    "href": "posts/hw1/hw1.html",
    "title": "HW 1",
    "section": "",
    "text": "Data Wrangling and Visualization\nThis work will create several interesting, interactive data graphics using the NOAA climate data.\n\n\nImporting Data\n\nimport pandas as pd\nimport sqlite3\nimport numpy as np\nimport plotly.express as px\nfrom sklearn.linear_model import LinearRegression\nimport plotly.io as pio\npio.renderers.default=\"iframe\"\n\n\n#connecting to sql, creating the database   \nconn = sqlite3.connect(\"data.db\")\ncursor = conn.cursor()\n\n#adding the countries data to the database\ncountries = pd.read_csv(\"hw1/countries.csv\")\ncountries.to_sql(\"countries\", conn, if_exists=\"replace\", index=False)\n\n#adding the stations data to the database\nstations = pd.read_csv(\"hw1/stations.csv\")\nstations.to_sql(\"stations\", conn, if_exists=\"replace\", index=False)\n\n#adding the temperature data to the database, removing rows without data\ntemperatures = pd.read_csv(\"hw1/temps.csv\")\ntemperatures = temperatures.dropna(subset=[\"Temp\"])\ntemperatures.to_sql(\"temperatures\", conn, if_exists=\"replace\", index=False)\n\n#committing and closing the connection\nconn.commit()\nconn.close()\n\n\n\nFirst Query Function\n\ndef query_climate_database(db_file, country, year_begin, year_end, month):\n    '''\n    Args:\n        db_file, the file name for the database\n        country, a string giving the name of a country for which data should be returned.\n        year_begin and year_end, two integers giving the earliest and latest years for which should be returned (inclusive).\n        month, an integer giving the month of the year for which should be returned.\n    Returns:\n        a Pandas dataframe of temperature readings for the specified country, in the specified date range, in the specified month \n        of the year with these columns: NAME, LATITUDE, LONGITUDE, Country, Year, Month, Temp.\n    '''\n    #creating the query\n    query = '''\n    SELECT S.NAME, S.LATITUDE, S.LONGITUDE, C.Name AS Country, T.Year, T.Month, T.Temp\n    FROM stations S\n    JOIN temperatures T ON S.ID = T.ID\n    JOIN countries C ON SUBSTR(T.ID, 1, 2) = C.\"FIPS 10-4\"\n    WHERE C.Name = ?\n    AND T.Year BETWEEN ? AND ?\n    AND T.Month = ?\n    ORDER BY S.Name, T.Year\n    '''\n    #connecting, issuing the query with the correct parameters\n    conn = sqlite3.connect(db_file)\n    query_df = pd.read_sql_query(query, conn, params=(country, year_begin, year_end, month))\n    conn.close()\n    return query_df\n\n\nquery_climate_database(db_file = \"data.db\", country = \"India\", year_begin = 1980, year_end = 2020, month = 1)\n\n\n\n\n\n\n\n\nNAME\nLATITUDE\nLONGITUDE\nCountry\nYear\nMonth\nTemp\n\n\n\n\n0\nAGARTALA\n23.883\n91.250\nIndia\n1980.0\n1.0\n18.21\n\n\n1\nAGARTALA\n23.883\n91.250\nIndia\n1981.0\n1.0\n18.25\n\n\n2\nAGARTALA\n23.883\n91.250\nIndia\n1982.0\n1.0\n19.31\n\n\n3\nAGARTALA\n23.883\n91.250\nIndia\n1985.0\n1.0\n19.25\n\n\n4\nAGARTALA\n23.883\n91.250\nIndia\n1988.0\n1.0\n19.54\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3147\nVISHAKHAPATNAM\n17.717\n83.233\nIndia\n2016.0\n1.0\n25.09\n\n\n3148\nVISHAKHAPATNAM\n17.717\n83.233\nIndia\n2017.0\n1.0\n23.90\n\n\n3149\nVISHAKHAPATNAM\n17.717\n83.233\nIndia\n2018.0\n1.0\n22.65\n\n\n3150\nVISHAKHAPATNAM\n17.717\n83.233\nIndia\n2019.0\n1.0\n22.20\n\n\n3151\nVISHAKHAPATNAM\n17.717\n83.233\nIndia\n2020.0\n1.0\n23.75\n\n\n\n\n3152 rows × 7 columns\n\n\n\n\nquery_climate_database(db_file = \"data.db\", country = \"Australia\", year_begin = 2000, year_end = 2020, month = 12)\n\n\n\n\n\n\n\n\nNAME\nLATITUDE\nLONGITUDE\nCountry\nYear\nMonth\nTemp\n\n\n\n\n0\nADELAIDE_AIRPORT\n-34.9524\n138.5204\nAustralia\n2000.0\n12.0\n21.20\n\n\n1\nADELAIDE_AIRPORT\n-34.9524\n138.5204\nAustralia\n2001.0\n12.0\n17.85\n\n\n2\nADELAIDE_AIRPORT\n-34.9524\n138.5204\nAustralia\n2002.0\n12.0\n21.05\n\n\n3\nADELAIDE_AIRPORT\n-34.9524\n138.5204\nAustralia\n2003.0\n12.0\n21.76\n\n\n4\nADELAIDE_AIRPORT\n-34.9524\n138.5204\nAustralia\n2004.0\n12.0\n20.51\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n13457\nYUNTA_AIRSTRIP\n-32.5707\n139.5645\nAustralia\n2013.0\n12.0\n23.41\n\n\n13458\nYUNTA_AIRSTRIP\n-32.5707\n139.5645\nAustralia\n2016.0\n12.0\n23.71\n\n\n13459\nYUNTA_AIRSTRIP\n-32.5707\n139.5645\nAustralia\n2017.0\n12.0\n23.38\n\n\n13460\nYUNTA_AIRSTRIP\n-32.5707\n139.5645\nAustralia\n2018.0\n12.0\n25.13\n\n\n13461\nYUNTA_AIRSTRIP\n-32.5707\n139.5645\nAustralia\n2020.0\n12.0\n21.98\n\n\n\n\n13462 rows × 7 columns\n\n\n\n\n\nGeographic Scatter Function for Yearly Temperature Increases\n\ndef temperature_coefficient_plot(db_file, country, year_begin, year_end, month, min_obs, **kwargs):\n    '''\n    Args:\n        db_file, the file name for the database\n        country, a string giving the name of a country for which data should be returned.\n        year_begin and year_end, two integers giving the earliest and latest years for which should be returned (inclusive).\n        month, an integer giving the month of the year for which should be returned.\n        min_obs, the minimum required number of years of data for any given station\n        **kwargs, additional keyword arguments passed to px.scatter_mapbox()\n    Returns:\n        An interactive geographic scatterplot with a point for each station \n        (the color of the point reflects an estimate of the yearly change in temperature during the specified time period)\n    '''\n    #creating queried dataframe\n    df = query_climate_database(db_file, country, year_begin, year_end, month)\n\n    #ensuring that each year has the minimum number of records in it\n    df['year_count'] = df.groupby(['NAME', 'LATITUDE', 'LONGITUDE'])['Year'].transform('nunique')\n    df = df[df['year_count'] &gt;= min_obs]\n\n    #coefficient function to find the Linear Regression coefficient for each group\n    def coef(data_group):\n        X = data_group[[\"Year\"]]\n        y = data_group[\"Temp\"]\n        LR = LinearRegression()\n        LR.fit(X,y)\n        slope = LR.coef_[0]\n        return slope\n\n    #grouping the data by the location and station\n    coefs = df.groupby(['NAME', 'LATITUDE', 'LONGITUDE']).apply(coef).reset_index(name='Estimated Yearly Increase (°C)')\n    coefs.dropna(subset=['Estimated Yearly Increase (°C)'], inplace=True)\n    coefs['Estimated Yearly Increase (°C)'] = coefs['Estimated Yearly Increase (°C)'].round(4)\n\n    #creating the scatter mapbox allowing for any key word arguments\n    fig = px.scatter_mapbox(\n        coefs,\n        lat=\"LATITUDE\",\n        lon=\"LONGITUDE\",\n        color=\"Estimated Yearly Increase (°C)\",\n        hover_name=\"NAME\",\n        title=f\"Estimates of yearly increases in temperature in January for stations in {country}, years {year_begin} - {year_end}\", \n        **kwargs, )\n    \n    return fig\n\n\nfig = temperature_coefficient_plot(\"data.db\", \"India\", 1980, 2020, 1, min_obs = 10, zoom = 2, mapbox_style=\"carto-positron\",\n                                       color_continuous_scale=px.colors.diverging.RdGy_r)\nfig.show()\n\n\n\n\n\nfig = temperature_coefficient_plot(\"data.db\", \"Australia\", 2000, 2020, 12, min_obs = 10, zoom = 2, mapbox_style=\"carto-positron\",\n                                       color_continuous_scale=px.colors.diverging.BrBG)\nfig.show()\n\n\n\nSecond Query Function\n\ndef query_climate_database_2(db_file, year_begin, year_end):\n    '''\n    Args:\n        db_file, the file name for the database\n        year_begin and year_end, two integers giving the earliest and latest years for which should be returned (inclusive).\n    Returns:\n        a Pandas dataframe of average temperature readings for each country during each month of in the specified year range\n        with these columns: Country, Temp, Year, Month\n    '''\n    #creating the query\n    query = '''\n    SELECT C.Name AS Country, AVG(T.Temp) as Temp, T.Year, T.Month\n    FROM temperatures T\n    JOIN countries C ON SUBSTR(T.ID, 1, 2) = C.\"FIPS 10-4\"\n    AND T.Year BETWEEN ? AND ?\n    GROUP BY C.Name, T.Year, T.Month\n    '''\n    #connecting, issuing the query with the correct parameters\n    conn = sqlite3.connect(db_file)\n    query_df = pd.read_sql_query(query, conn, params=(year_begin, year_end))\n    conn.close()\n    return query_df\n\n\n\nAdditional Graphs\n\ndef temperature_choropleth(db_file, year, **kwargs):\n    '''\n    Answers the following question: How does the average temperature per country differ in a particular year?\n    Args:\n        db_file, the file name for the database\n        year, integer giving the year for which should be returned\n        **kwargs, additional keyword arguments passed to px.scatter()\n    Returns:\n        An interactive choropleth with a point for each country\n        (color is represented by the average temperature at each country)\n    '''\n    #creating the dataframe with the data\n    data = query_climate_database_2(db_file, year, year)\n    \n    #finding the mean temperature of each country using all of the data\n    df = data.groupby(\"Country\")[\"Temp\"].mean().reset_index()    \n    df[\"Temp\"] = df[\"Temp\"].round(4)\n    \n    #plotting the data on a choropleth to see the colors indicating the average temperatures per country\n    fig = px.choropleth(\n        df,\n        locations=\"Country\",\n        locationmode = \"country names\",\n        color=\"Temp\",\n        title=f\"Average Temperature per Country in {year}\",\n        labels={\"Temp\": \"Average Temp (°C)\"},\n        **kwargs,\n    )\n    return fig\n\n\nfig = temperature_choropleth(\"data.db\", 2020)\nfig.show()\n\n\ndef temperature_bar_graph(db_file, year_begin, year_end, **kwargs):\n    '''\n    Answers the following question: How does the average temperature globally per month differ in a range of years?\n    Args:\n        db_file, the file name for the database\n        year, integer giving the year for which should be returned\n        **kwargs, additional keyword arguments passed to px.scatter()\n    Returns:\n        An interactive bar graph with the average global temperatures of each month facetted by year\n    '''\n    #creating a data frame with the necessary data\n    data = query_climate_database_2(db_file, year_begin, year_end)\n\n    #grouping the data by the years and months, then finding the mean temperature for each year-month combination\n    df = data.groupby([\"Year\", \"Month\"])[\"Temp\"].mean().reset_index()\n    df['Year'] = df['Year'].astype(int)\n    df[\"Temp\"] = df[\"Temp\"].round(4)\n\n    #mapping names for the numerical month values\n    months = {1: \"January\", 2: \"February\", 3: \"March\", 4: \"April\", 5: \"May\", 6: \"June\", 7: \"July\", 8: \"August\", 9: \"September\", 10: \"October\",  11: \"November\", 12: \"December\"}\n    df['Months'] = df['Month'].map(months)\n\n    #creating a bar chart\n    fig = px.bar(\n        df,\n        x=\"Months\",\n        y=\"Temp\",\n        title=f\"Average Temperature Across All Countries from {year_begin} to {year_end}\",\n        labels={\"Temp\": \"Average Temp (°C)\"},\n        facet_col=\"Year\",  # Facet by year\n        **kwargs,\n    )\n    \n    return fig\n\n\nfig = temperature_bar_graph(\"data.db\", 2016, 2018)\nfig.show()\n\n\ndef temperature_line_graph(db_file, year_begin, year_end, **kwargs):\n    '''\n    Answers the following question: How does the average temperature globally per month differ in a range of years?\n    Args:\n        db_file, the file name for the database\n        year, integer giving the year for which should be returned\n        **kwargs, additional keyword arguments passed to px.scatter()\n    Returns:\n        An interactive line graph with the average temperatures per month each year\n    '''\n\n    #creating a data frame with the necessary data\n    data = query_climate_database_2(db_file, year_begin, year_end)\n\n    #grouping the data by the year and month, finding the average temperature for each year-month combination\n    df = data.groupby([\"Year\", \"Month\"])[\"Temp\"].mean().reset_index()  \n    df['Year'] = df['Year'].astype(int)\n    df[\"Temp\"] = df[\"Temp\"].round(4)\n\n    #mapping the months to their names because of their numerical values\n    months = {1: \"January\", 2: \"February\", 3: \"March\", 4: \"April\", 5: \"May\", 6: \"June\", 7: \"July\", 8: \"August\", 9: \"September\", 10: \"October\",  11: \"November\", 12: \"December\"}\n    df['Months'] = df['Month'].map(months)\n\n    #creating a line graph with overlapping lines, each representing one of the years, allowing any key word arguments given\n    fig = px.line(\n        df,\n        x=\"Months\",\n        y=\"Temp\",\n        color=\"Year\",\n        title=f\"Average Temperature Across All Countries from {year_begin} to {year_end}\",\n        labels={\"Temp\": \"Average Temp (°C)\"},\n        **kwargs,\n    )\n    return fig\n\n\nfig = temperature_line_graph(\"data.db\", 2010, 2020)\nfig.show()"
  },
  {
    "objectID": "posts/bruin/index.html",
    "href": "posts/bruin/index.html",
    "title": "HW 0",
    "section": "",
    "text": "This tutorial will show and explain the code to analyze a dataframe through the creation of a visualization. It will use the Palmer’s Penguin dataset, but these strategies can be expanded for other dataframes and visualization choices.\n\n\nIn order to create an interesting visualization of the Palmer Penguins data set, it is first necessary to import the data and necessary packages. For this specific visualization, I imported the pandas, matplotlib, and seaborn libraries. The pandas library will allow us to convert the data into a data frame that can be analyzed. Using matplotlib and seaborn, I can create graphs that will depict the chosen features and aspects of the graphs.\n\n#import the necessary libraries\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\n\n#create a data frame using the data and pandas library\nurl = \"https://raw.githubusercontent.com/pic16b-ucla/24W/main/datasets/palmer_penguins.csv\"\npenguins = pd.read_csv(url)\n\n\n#visualizing the data frame allows us to see the different features\npenguins.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0708\n1\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A1\nYes\n11/11/07\n39.1\n18.7\n181.0\n3750.0\nMALE\nNaN\nNaN\nNot enough blood for isotopes.\n\n\n1\nPAL0708\n2\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A2\nYes\n11/11/07\n39.5\n17.4\n186.0\n3800.0\nFEMALE\n8.94956\n-24.69454\nNaN\n\n\n2\nPAL0708\n3\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A1\nYes\n11/16/07\n40.3\n18.0\n195.0\n3250.0\nFEMALE\n8.36821\n-25.33302\nNaN\n\n\n3\nPAL0708\n4\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A2\nYes\n11/16/07\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nAdult not sampled.\n\n\n4\nPAL0708\n5\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN3A1\nYes\n11/16/07\n36.7\n19.3\n193.0\n3450.0\nFEMALE\n8.76651\n-25.32426\nNaN\n\n\n\n\n\n\n\n\n\n\nThe next step to create these visualizations is to decide which features of the dataset that you would want to illustration. I personally chose to see how the body mass of penguins differs depending on their sex for each species. Because I now know the features I want to analyze, I can isolate each of those columns for future steps.\n\n#create a copy of the data frame with chosen features\ndf = penguins[[\"Sex\", \"Body Mass (g)\", \"Species\"]].copy()\ndf.head()\n\n\n\n\n\n\n\n\nSex\nBody Mass (g)\nSpecies\n\n\n\n\n0\nMALE\n3750.0\nAdelie Penguin (Pygoscelis adeliae)\n\n\n1\nFEMALE\n3800.0\nAdelie Penguin (Pygoscelis adeliae)\n\n\n2\nFEMALE\n3250.0\nAdelie Penguin (Pygoscelis adeliae)\n\n\n3\nNaN\nNaN\nAdelie Penguin (Pygoscelis adeliae)\n\n\n4\nFEMALE\n3450.0\nAdelie Penguin (Pygoscelis adeliae)\n\n\n\n\n\n\n\n\n\n\nThe next step in the process is cleaning the relevant data by removing any penguins subjects with any missing measurements. Now that the data frame only contains the features for the graph, we can use the dropna function, which removes the rows with any null values. Had we done this before we removed the unnecessary columns, we could have unnecessarily removed rows where irrelavent columns didn’t have data. Also, we have to ensure that the Sex values are either Male or Female as there was one penguin without a null value or valid one. This cleanup removed 11 of the penguins from the visualization but ensured that the ones which were represented contained all the necessary information. I also changed the species labels to just include the species name rather than the excessive “Species” ending in the title for better readability.\n\n# Modify the data frame to make sure that the sex values are male or female\ndf = df[df[\"Sex\"].isin([\"MALE\", \"FEMALE\"])]\n# Modify the data frame to make sure that body mass and species have correct values\ndf.dropna(inplace=True)\n\n# Shorten the species names to avoid redundancy\ndf[\"Species\"] = df[\"Species\"].str.split().str[0]\n\n# Display the first few rows\ndf.head()\n\n\n\n\n\n\n\n\nSex\nBody Mass (g)\nSpecies\n\n\n\n\n0\nMALE\n3750.0\nAdelie\n\n\n1\nFEMALE\n3800.0\nAdelie\n\n\n2\nFEMALE\n3250.0\nAdelie\n\n\n4\nFEMALE\n3450.0\nAdelie\n\n\n5\nMALE\n3650.0\nAdelie\n\n\n\n\n\n\n\n\n\n\nThe next step is deciding how to represent your chosen data. I personally wanted to study the range of the body mass for each sex and species of the penguins and therefore decided to use boxplots that were vertically stacked to see how the ranged differed. Creating a figure with two subplots sharing their x-axis of body mass in grams, I was able to plot the data. I created a function that would create the boxplots for the data considering which axis and sex were passed as parameters for the function to prevent redundancy. Using the cleaned data, the function would split the data depending on the sex. Setting the y-axis to be the species of the penguins and the titles of each of the subplots to represent each of the sexes, the visualization allows us to see how the body masses of penguins ranged for each species considering their sex.\n\n#create the figure with two vertical subplots that share their x-axis\nfig, ax = plt.subplots(2, 1, figsize=(6, 6), sharex=True)\n\ndef create_boxplot(i, sex):\n    '''\n    Creates two boxplots for each sex on each axis\n    Args:\n        i (int): axes number for the data visualization (0 or 1)\n        sex: which sex to represent in the subplot ('MALE' or 'FEMALE')\n    Returns:\n        None\n    '''\n    #divide the data\n    data = df[df[\"Sex\"] == sex]\n    #create the boxplot for the set data on the correct axis i\n    sns.boxplot(data=data, x=\"Body Mass (g)\", y=\"Species\", hue=\"Species\", width=0.6, dodge = False, ax=ax[i])\n\n#create the boxplots for each sex\ncreate_boxplot(0, \"MALE\")\ncreate_boxplot(1, \"FEMALE\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nOnce the graphs are created, extra aesthetic features can be added to improve the readability of the graphs. One such change was creating palette with three colors (each representing one of the species) using the seaborn library and applying the changes to each of the boxplots. The next change was setting the theme of the seaborn graphs to a standard whitegrid. This allowed for the creation of gridlines and clear borders to visualize the differences between the six boxplots better. Finally, adding labels for the axes and and titles for each of the subplots in addition to the legends representing the species colors improved the readability of the graph overall.\n\n#create the color palette for the graphs using seaborn library color schemes\npalette = sns.color_palette(\"flare\", n_colors=3)\n#set the the theme and font scale of the graphs using the seaborn library\nsns.set_theme(style='whitegrid', font_scale=1.0)\n\n#create the figure with two vertical subplots that share their x-axis\nfig, ax = plt.subplots(2, 1, figsize=(6, 6), sharex=True)\n\ndef create_boxplot(i, sex):\n    '''\n    Creates two boxplots for each sex on each axis\n    Args:\n        i (int): axes number for the data visualization (0 or 1)\n        sex: which sex to represent in the subplot ('MALE' or 'FEMALE')\n    Returns:\n        None\n    '''\n    #divide the data\n    data = df[df[\"Sex\"] == sex]\n    #create the boxplot for the set data on the correct axis i\n    sns.boxplot(data=data, x=\"Body Mass (g)\", y=\"Species\", hue=\"Species\",palette=palette, width=0.6, dodge = False, ax=ax[i])\n\n#create the boxplots for each sex\ncreate_boxplot(0, \"MALE\")\ncreate_boxplot(1, \"FEMALE\")\n\n#set the labels and titles of the graphs\nax[0].set(xlabel=\"\", title=\"Male\", ylabel=\"Species\")\nax[1].set(ylabel=\"Species\", xlabel = \"Body Mass (g)\", title=\"Female\")\n\n#format the layout of the graph and display\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nOverall, these are the basic steps to create an interesting data visualization of the Palmer Penguins data set. There are many ways to represent the data of your choice. Using the matplotlib and seaborn libraries, you can choose from a variety of graphs that will allow you to interpret your data. Although I chose to represent one feature considering two variables, there are a multitude of options to choose from. In the future, using these steps will allow one to create diverse and interesting data visualizations!\n\n\n\n\n#import the necessary libraries\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\n#create a data frame using the data and pandas library\nurl = \"https://raw.githubusercontent.com/pic16b-ucla/24W/main/datasets/palmer_penguins.csv\"\npenguins = pd.read_csv(url)\n\n#create a copy of the data frame with chosen features\ndf = penguins[[\"Sex\", \"Body Mass (g)\", \"Species\"]].copy()\n\n#modify the data frame to make sure that the sex values are male or female\ndf = df[df[\"Sex\"].isin([\"MALE\", \"FEMALE\"])]\n#modify the data frame to make sure that body mass and species have correct values\ndf.dropna(inplace=True)\n\n# Shorten the species names to avoid redundancy\ndf[\"Species\"] = df[\"Species\"].str.split().str[0]\n\n#create the color palette for the graphs using seaborn library color schemes\npalette = sns.color_palette(\"flare\", n_colors=3)\n#set the the theme and font scale of the graphs using the seaborn library\nsns.set_theme(style='whitegrid', font_scale=1.0)\n\n#create the figure with two vertical subplots that share their x-axis\nfig, ax = plt.subplots(2, 1, figsize=(6, 6), sharex=True)\n\ndef create_boxplot(i, sex):\n    '''\n    Creates two boxplots for each sex on each axis\n    Args:\n        i (int): axes number for the data visualization (0 or 1)\n        sex: which sex to represent in the subplot ('MALE' or 'FEMALE')\n    Returns:\n        None\n    '''\n    #divide the data\n    data = df[df[\"Sex\"] == sex]\n    #create the boxplot for the set data on the correct axis i\n    sns.boxplot(data=data, x=\"Body Mass (g)\", y=\"Species\", hue=\"Species\",palette=palette, width=0.6, dodge = False, ax=ax[i])\n\n#create the boxplots for each sex\ncreate_boxplot(0, \"MALE\")\ncreate_boxplot(1, \"FEMALE\")\n\n#set the labels and titles of the graphs\nax[0].set(xlabel=\"\", title=\"Male\", ylabel=\"Species\")\nax[1].set(ylabel=\"Species\", xlabel = \"Body Mass (g)\", title=\"Female\")\n\n#format the layout of the graph and display\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/bruin/index.html#preliminary-setup",
    "href": "posts/bruin/index.html#preliminary-setup",
    "title": "HW 0",
    "section": "",
    "text": "In order to create an interesting visualization of the Palmer Penguins data set, it is first necessary to import the data and necessary packages. For this specific visualization, I imported the pandas, matplotlib, and seaborn libraries. The pandas library will allow us to convert the data into a data frame that can be analyzed. Using matplotlib and seaborn, I can create graphs that will depict the chosen features and aspects of the graphs.\n\n#import the necessary libraries\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\n\n#create a data frame using the data and pandas library\nurl = \"https://raw.githubusercontent.com/pic16b-ucla/24W/main/datasets/palmer_penguins.csv\"\npenguins = pd.read_csv(url)\n\n\n#visualizing the data frame allows us to see the different features\npenguins.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0708\n1\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A1\nYes\n11/11/07\n39.1\n18.7\n181.0\n3750.0\nMALE\nNaN\nNaN\nNot enough blood for isotopes.\n\n\n1\nPAL0708\n2\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A2\nYes\n11/11/07\n39.5\n17.4\n186.0\n3800.0\nFEMALE\n8.94956\n-24.69454\nNaN\n\n\n2\nPAL0708\n3\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A1\nYes\n11/16/07\n40.3\n18.0\n195.0\n3250.0\nFEMALE\n8.36821\n-25.33302\nNaN\n\n\n3\nPAL0708\n4\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A2\nYes\n11/16/07\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nAdult not sampled.\n\n\n4\nPAL0708\n5\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN3A1\nYes\n11/16/07\n36.7\n19.3\n193.0\n3450.0\nFEMALE\n8.76651\n-25.32426\nNaN"
  },
  {
    "objectID": "posts/bruin/index.html#data-selection",
    "href": "posts/bruin/index.html#data-selection",
    "title": "HW 0",
    "section": "",
    "text": "The next step to create these visualizations is to decide which features of the dataset that you would want to illustration. I personally chose to see how the body mass of penguins differs depending on their sex for each species. Because I now know the features I want to analyze, I can isolate each of those columns for future steps.\n\n#create a copy of the data frame with chosen features\ndf = penguins[[\"Sex\", \"Body Mass (g)\", \"Species\"]].copy()\ndf.head()\n\n\n\n\n\n\n\n\nSex\nBody Mass (g)\nSpecies\n\n\n\n\n0\nMALE\n3750.0\nAdelie Penguin (Pygoscelis adeliae)\n\n\n1\nFEMALE\n3800.0\nAdelie Penguin (Pygoscelis adeliae)\n\n\n2\nFEMALE\n3250.0\nAdelie Penguin (Pygoscelis adeliae)\n\n\n3\nNaN\nNaN\nAdelie Penguin (Pygoscelis adeliae)\n\n\n4\nFEMALE\n3450.0\nAdelie Penguin (Pygoscelis adeliae)"
  },
  {
    "objectID": "posts/bruin/index.html#data-cleaning",
    "href": "posts/bruin/index.html#data-cleaning",
    "title": "HW 0",
    "section": "",
    "text": "The next step in the process is cleaning the relevant data by removing any penguins subjects with any missing measurements. Now that the data frame only contains the features for the graph, we can use the dropna function, which removes the rows with any null values. Had we done this before we removed the unnecessary columns, we could have unnecessarily removed rows where irrelavent columns didn’t have data. Also, we have to ensure that the Sex values are either Male or Female as there was one penguin without a null value or valid one. This cleanup removed 11 of the penguins from the visualization but ensured that the ones which were represented contained all the necessary information. I also changed the species labels to just include the species name rather than the excessive “Species” ending in the title for better readability.\n\n# Modify the data frame to make sure that the sex values are male or female\ndf = df[df[\"Sex\"].isin([\"MALE\", \"FEMALE\"])]\n# Modify the data frame to make sure that body mass and species have correct values\ndf.dropna(inplace=True)\n\n# Shorten the species names to avoid redundancy\ndf[\"Species\"] = df[\"Species\"].str.split().str[0]\n\n# Display the first few rows\ndf.head()\n\n\n\n\n\n\n\n\nSex\nBody Mass (g)\nSpecies\n\n\n\n\n0\nMALE\n3750.0\nAdelie\n\n\n1\nFEMALE\n3800.0\nAdelie\n\n\n2\nFEMALE\n3250.0\nAdelie\n\n\n4\nFEMALE\n3450.0\nAdelie\n\n\n5\nMALE\n3650.0\nAdelie"
  },
  {
    "objectID": "posts/bruin/index.html#creating-the-graphs",
    "href": "posts/bruin/index.html#creating-the-graphs",
    "title": "HW 0",
    "section": "",
    "text": "The next step is deciding how to represent your chosen data. I personally wanted to study the range of the body mass for each sex and species of the penguins and therefore decided to use boxplots that were vertically stacked to see how the ranged differed. Creating a figure with two subplots sharing their x-axis of body mass in grams, I was able to plot the data. I created a function that would create the boxplots for the data considering which axis and sex were passed as parameters for the function to prevent redundancy. Using the cleaned data, the function would split the data depending on the sex. Setting the y-axis to be the species of the penguins and the titles of each of the subplots to represent each of the sexes, the visualization allows us to see how the body masses of penguins ranged for each species considering their sex.\n\n#create the figure with two vertical subplots that share their x-axis\nfig, ax = plt.subplots(2, 1, figsize=(6, 6), sharex=True)\n\ndef create_boxplot(i, sex):\n    '''\n    Creates two boxplots for each sex on each axis\n    Args:\n        i (int): axes number for the data visualization (0 or 1)\n        sex: which sex to represent in the subplot ('MALE' or 'FEMALE')\n    Returns:\n        None\n    '''\n    #divide the data\n    data = df[df[\"Sex\"] == sex]\n    #create the boxplot for the set data on the correct axis i\n    sns.boxplot(data=data, x=\"Body Mass (g)\", y=\"Species\", hue=\"Species\", width=0.6, dodge = False, ax=ax[i])\n\n#create the boxplots for each sex\ncreate_boxplot(0, \"MALE\")\ncreate_boxplot(1, \"FEMALE\")\n\nplt.show()"
  },
  {
    "objectID": "posts/bruin/index.html#extra-graph-features",
    "href": "posts/bruin/index.html#extra-graph-features",
    "title": "HW 0",
    "section": "",
    "text": "Once the graphs are created, extra aesthetic features can be added to improve the readability of the graphs. One such change was creating palette with three colors (each representing one of the species) using the seaborn library and applying the changes to each of the boxplots. The next change was setting the theme of the seaborn graphs to a standard whitegrid. This allowed for the creation of gridlines and clear borders to visualize the differences between the six boxplots better. Finally, adding labels for the axes and and titles for each of the subplots in addition to the legends representing the species colors improved the readability of the graph overall.\n\n#create the color palette for the graphs using seaborn library color schemes\npalette = sns.color_palette(\"flare\", n_colors=3)\n#set the the theme and font scale of the graphs using the seaborn library\nsns.set_theme(style='whitegrid', font_scale=1.0)\n\n#create the figure with two vertical subplots that share their x-axis\nfig, ax = plt.subplots(2, 1, figsize=(6, 6), sharex=True)\n\ndef create_boxplot(i, sex):\n    '''\n    Creates two boxplots for each sex on each axis\n    Args:\n        i (int): axes number for the data visualization (0 or 1)\n        sex: which sex to represent in the subplot ('MALE' or 'FEMALE')\n    Returns:\n        None\n    '''\n    #divide the data\n    data = df[df[\"Sex\"] == sex]\n    #create the boxplot for the set data on the correct axis i\n    sns.boxplot(data=data, x=\"Body Mass (g)\", y=\"Species\", hue=\"Species\",palette=palette, width=0.6, dodge = False, ax=ax[i])\n\n#create the boxplots for each sex\ncreate_boxplot(0, \"MALE\")\ncreate_boxplot(1, \"FEMALE\")\n\n#set the labels and titles of the graphs\nax[0].set(xlabel=\"\", title=\"Male\", ylabel=\"Species\")\nax[1].set(ylabel=\"Species\", xlabel = \"Body Mass (g)\", title=\"Female\")\n\n#format the layout of the graph and display\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/bruin/index.html#final-notes",
    "href": "posts/bruin/index.html#final-notes",
    "title": "HW 0",
    "section": "",
    "text": "Overall, these are the basic steps to create an interesting data visualization of the Palmer Penguins data set. There are many ways to represent the data of your choice. Using the matplotlib and seaborn libraries, you can choose from a variety of graphs that will allow you to interpret your data. Although I chose to represent one feature considering two variables, there are a multitude of options to choose from. In the future, using these steps will allow one to create diverse and interesting data visualizations!"
  },
  {
    "objectID": "posts/bruin/index.html#final-code",
    "href": "posts/bruin/index.html#final-code",
    "title": "HW 0",
    "section": "",
    "text": "#import the necessary libraries\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\n#create a data frame using the data and pandas library\nurl = \"https://raw.githubusercontent.com/pic16b-ucla/24W/main/datasets/palmer_penguins.csv\"\npenguins = pd.read_csv(url)\n\n#create a copy of the data frame with chosen features\ndf = penguins[[\"Sex\", \"Body Mass (g)\", \"Species\"]].copy()\n\n#modify the data frame to make sure that the sex values are male or female\ndf = df[df[\"Sex\"].isin([\"MALE\", \"FEMALE\"])]\n#modify the data frame to make sure that body mass and species have correct values\ndf.dropna(inplace=True)\n\n# Shorten the species names to avoid redundancy\ndf[\"Species\"] = df[\"Species\"].str.split().str[0]\n\n#create the color palette for the graphs using seaborn library color schemes\npalette = sns.color_palette(\"flare\", n_colors=3)\n#set the the theme and font scale of the graphs using the seaborn library\nsns.set_theme(style='whitegrid', font_scale=1.0)\n\n#create the figure with two vertical subplots that share their x-axis\nfig, ax = plt.subplots(2, 1, figsize=(6, 6), sharex=True)\n\ndef create_boxplot(i, sex):\n    '''\n    Creates two boxplots for each sex on each axis\n    Args:\n        i (int): axes number for the data visualization (0 or 1)\n        sex: which sex to represent in the subplot ('MALE' or 'FEMALE')\n    Returns:\n        None\n    '''\n    #divide the data\n    data = df[df[\"Sex\"] == sex]\n    #create the boxplot for the set data on the correct axis i\n    sns.boxplot(data=data, x=\"Body Mass (g)\", y=\"Species\", hue=\"Species\",palette=palette, width=0.6, dodge = False, ax=ax[i])\n\n#create the boxplots for each sex\ncreate_boxplot(0, \"MALE\")\ncreate_boxplot(1, \"FEMALE\")\n\n#set the labels and titles of the graphs\nax[0].set(xlabel=\"\", title=\"Male\", ylabel=\"Species\")\nax[1].set(ylabel=\"Species\", xlabel = \"Body Mass (g)\", title=\"Female\")\n\n#format the layout of the graph and display\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/hw2/hw2.html",
    "href": "posts/hw2/hw2.html",
    "title": "HW 2",
    "section": "",
    "text": "Web Scraping\nThe first step in the process of web scraping is creating a scrapy project. This may involve pip installing the framework in the terminal and then creating a project with the line, “scrapy startproject TMDB_scraper.” From there, it is important that you set your current directory to that project using “cd TMDB_scraper.” From there, create the file “tmdb_spider.py” within the spiders folder of the project which will be automatically created. From there, we begin coding in that file…\n\n\nUsing Scrapy to Create a Spider\nFirst, import the necessary scrapy library for this project\n\nimport scrapy\n\nNext, we will create a class that will create a spider to parse through whichever URL we provide it on initialization.\n\nclass TmdbSpider(scrapy.Spider):\n    name = 'tmdb_spider'\n\nWithin the init method, we will initialize the Spider with the correct subdir argument in order to ensure that the URL will go to the directory for the correct show or movie. Also, we will initialize the Spider parent class member. We will also create a list of the starting URLs for the Scrapy within the object.\n\ndef __init__(self, subdir=\"\", *args, **kwargs):\n    '''\n    Initializes a TmdbSpider object and creates the starting URL\n    Args:\n        subdir (string): the movie id to go to designated URL\n        *args, **kwargs\n    Returns: \n        None\n    '''\n    self.start_urls = [f\"https://www.themoviedb.org/movie/{subdir}/\"]\n    #initializing the scrapy.Spider object\n    super().__init__(*args, **kwargs)\n\nIn the parse method, we will ensure that the default URL is correctly set up and allow for the user to see the full movie setup. From there it will navigate to the cast of the movie by appending “/cast” to the original movie URL. The function will yield a scrapy Request meaning that it will pass the new URL into the designated function, which is the full credits one in this case. This is called a callback function meaning that it will be called when response for that request is answered.\n\ndef parse(self, response):\n    '''\n    Navigates to the Full Cast & Crew Page of the movie.\n    Args:\n        response (scrapy.http.Response): response object from initialization\n    Yields:\n        scrapy.Request: Request to navigate to the Full Cast & Crew Page of the movie\n    '''\n    #creates new cast URL\n    url = response.url + \"/cast\"\n    yield scrapy.Request(url=url, callback=self.parse_full_credits)\n\nIn the parse full credits function, the method processes the response from the Full Cast and Crew page. It goes through the list of actors which is seen within the ol or ordered list tag. Each of the actors within the list have their specific links to their individual pages in the href attribute which is found in each li or list item and within the a tags of each one. Although this may seem confusing, by using the developer tools to examine the website’s layout, we can see which subcategories lead to the specific information we are seeking. Hovering over the correct code will highlight the information we seek, helping us create the css selector. Using this css selector on the response will generate a list of each of the actors individual page link subdirs. From there, we can go through each of the link endings and join them to the main one, yielding the Requests for each of the actor’s urls and calling back the next function, parse_actor_page.\n\ndef parse_full_credits(self,response):\n    '''\n    Produces a list of actors from the Full Cast & Crew page and goes to each actor's page.\n    Args:\n        response (scrapy.http.Response): response object from the Full Cast & Crew page\n    Yields:\n        scrapy.Request: request object to navigate to each actor's individual page\n    '''\n    #creates a list of all the actors\n    actors = response.css(\"ol.people.credits li a::attr(href)\").getall()\n    for actor in actors:\n        #joins the response url with each of the actor hrefs\n        url = response.urljoin(actor)\n        yield scrapy.Request(url=url, callback=self.parse_actor_page)\n\nWithin this function, we take the response to extract an actor’s name and produce a dictionary with each of the movies they have starred in. We do this first by finding the actor’s name which is found within the title div, under the h2.title in the a tag. Then we isolate each of the move titles, creating a list by using the getall() function instead of simply get(). They can be found in the credits list bdi tag. From there, we can parse through each of the unique movies for each actor by making the titles into a set to remove duplicates. This will allow us to yield each of the movie names with the actor name as the key and create a helpful dictionary.\n\ndef parse_actor_page(self, response):\n    '''\n    Deciphers an actor's name and the names of each of the movies they acted in.\n    Args:\n        response (scrapy.http.Response): response object from the actor's individual page\n    Yields:\n        dict: a dictionary of the actor's name with each of the movies they starred in\n    '''\n    #extracts the actor name from their page\n    actor_name = response.css(\"div.title h2.title a::text\").get()\n    #creates a list of all the movies that the actor starred in\n    movie_titles = response.css(\"div.credits_list bdi::text\").getall()\n    #for each of the unique movies, creates a dictionary with the movie and the actor name\n    for title in set(movie_titles):\n        yield {\"actor\": actor_name, \"movie_or_TV_name\": title}\n\nFinally, after creating each of the functions, we can put all of the code together into one piece:\n\nimport scrapy\n\nclass TmdbSpider(scrapy.Spider):\n    name = 'tmdb_spider'\n\n    def __init__(self, subdir=\"\", *args, **kwargs):\n        '''\n        Initializes a TmdbSpider object and creates the starting URL\n        Args:\n            subdir (string): the movie id to go to designated URL\n            *args, **kwargs\n        Returns: \n            None\n        '''\n        self.start_urls = [f\"https://www.themoviedb.org/movie/{subdir}/\"]\n        #initializing the scrapy.Spider object\n        super().__init__(*args, **kwargs)\n        \n    def parse(self, response):\n        '''\n        Navigates to the Full Cast & Crew Page of the movie.\n        Args:\n            response (scrapy.http.Response): response object from initialization\n        Yields:\n            scrapy.Request: Request to navigate to the Full Cast & Crew Page of the movie\n        '''\n        #creates new cast URL\n        url = response.url + \"/cast\"\n        yield scrapy.Request(url=url, callback=self.parse_full_credits)\n    \n    def parse_full_credits(self,response):\n        '''\n        Produces a list of actors from the Full Cast & Crew page and goes to each actor's page.\n        Args:\n            response (scrapy.http.Response): response object from the Full Cast & Crew page\n        Yields:\n            scrapy.Request: request object to navigate to each actor's individual page\n        '''\n        #creates a list of all the actors\n        actors = response.css(\"ol.people.credits li a::attr(href)\").getall()\n        for actor in actors:\n            #joins the response url with each of the actor hrefs\n            url = response.urljoin(actor)\n            yield scrapy.Request(url=url, callback=self.parse_actor_page)\n    \n    def parse_actor_page(self, response):\n        '''\n        Deciphers an actor's name and the names of each of the movies they acted in.\n        Args:\n            response (scrapy.http.Response): response object from the actor's individual page\n        Yields:\n            dict: a dictionary of the actor's name with each of the movies they starred in\n        '''\n        #extracts the actor name from their page\n        actor_name = response.css(\"div.title h2.title a::text\").get()\n        #creates a list of all the moveis that the actor starred in\n        movie_titles = response.css(\"div.credits_list bdi::text\").getall()\n        #for each of the unique movies, creates a dictionary with the movie and the actor name\n        for title in set(movie_titles):\n            yield {\"actor\": actor_name, \"movie_or_TV_name\": title}\n\nNow that we have created the project and our Spider class, we can work with scrapy to scrape different URLs. Functions in the terminal such as “scrapy crawl tmdb_spider -o results.csv -a subdir={movie_id}” where {movie_id} is replaced with the correct subdir for the moviedb page will create a csv file with the data compiled from the functions we wrote.\n\n\nCreating a Visualization\nFirst, import the correct libraries to convert our csv file into a pandas dataframe, which will make it easier to work with. Using the data from the CSV file produced by the spider, we can find the number of shared actors for each show or movie by counting the number of times that the particular work appears within the pandas dataframe. From there, we can assign the correct label names, producing an adequate data frame for our purposes.\n\nimport pandas as pd\nimport plotly.express as px\n\n\nresults = pd.read_csv(\"results.csv\")\nshared_actors = results[\"movie_or_TV_name\"].value_counts().reset_index()\nshared_actors.columns = ['Movie/Show Name', 'Number of Shared Actors']\nshared_actors.head(15)\n\n\n\n\n\n\n\n\nMovie/Show Name\nNumber of Shared Actors\n\n\n\n\n0\nCrazy Rich Asians\n141\n\n\n1\nChina Rich Girlfriend\n9\n\n\n2\nThe Forever Purge\n7\n\n\n3\nMulan\n7\n\n\n4\nRich People Problems\n6\n\n\n5\nIn the Heights\n5\n\n\n6\nThe Kelly Clarkson Show\n5\n\n\n7\nThe Black Phone\n5\n\n\n8\nNow You See Me 2\n5\n\n\n9\nBlackhat\n5\n\n\n10\nKung Fu Panda 4\n5\n\n\n11\nFifty Shades Freed\n5\n\n\n12\nWicked\n5\n\n\n13\nThe Daily Show\n5\n\n\n14\nThe Tonight Show Starring Jimmy Fallon\n4\n\n\n\n\n\n\n\nUsing the plotly express library, we can create an interesting visualization to see the range of shared actors in movies or shows in which Crazy Rich Asians actors are featured. In this case, I chose to use a bar graph, but there are many other libraries like matplotlib and seaborn as well as types of graphs which can be used to display your data.\n\nfig = px.bar(shared_actors.loc[1:16], x='Movie/Show Name', y='Number of Shared Actors', \n             title='Top 15 Movies/Shows Sharing Actors with Crazy Rich Asians ', \n             color='Number of Shared Actors', color_continuous_scale='Sunset')\nfig.show()"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome! I will add my projects here!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "myblog",
    "section": "",
    "text": "HW 2\n\n\n\n\n\n\nweek 6\n\n\nhomework\n\n\n\n\n\n\n\n\n\nFeb 7, 2025\n\n\nAnika Savla\n\n\n\n\n\n\n\n\n\n\n\n\nHW 1\n\n\n\n\n\n\nweek 5\n\n\nhomework\n\n\n\n\n\n\n\n\n\nFeb 3, 2025\n\n\nAnika Savla\n\n\n\n\n\n\n\n\n\n\n\n\nHW 0\n\n\n\n\n\n\nweek 3\n\n\nhomework\n\n\n\n\n\n\n\n\n\nJan 23, 2025\n\n\nAnika Savla\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nIntroduction\n\n\n\n\n\n\n\n\n\nJan 21, 2025\n\n\nAnika Savla\n\n\n\n\n\n\nNo matching items"
  }
]